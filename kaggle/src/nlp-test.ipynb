{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-15T13:37:15.994811Z",
     "iopub.status.busy": "2025-11-15T13:37:15.994494Z",
     "iopub.status.idle": "2025-11-15T13:37:18.652606Z",
     "shell.execute_reply": "2025-11-15T13:37:18.65132Z",
     "shell.execute_reply.started": "2025-11-15T13:37:15.99478Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Requirement already satisfied: pycountry in /home/mizu/KaggleChallenge/.venv/lib/python3.11/site-packages (24.6.1)\n",
      "Requirement already satisfied: pycountry in /home/mizu/KaggleChallenge/.venv/lib/python3.11/site-packages (24.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 各種ライブラリと CSV データの読み込み\n",
    "# ==========================================\n",
    "# 自動リロードを有効にする設定\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# 必要なライブラリのインストール\n",
    "%pip install pycountry\n",
    "\n",
    "# ライブラリのインポート\n",
    "import numpy as np  # 線形代数\n",
    "import pandas as pd  # データ処理、CSVファイルのI/O（例：pd.read_csv）\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 独自モジュールのインポート\n",
    "from modules.normalizer import TextNormalizer, AdditionalNormalizer\n",
    "\n",
    "# CSVデータを pandas データフレームオブジェクトとして読み込み\n",
    "df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\n",
    "df = df.fillna(\"\")  # 空のカラムを空文字に置換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# \"text\" カラムの前処理\n",
    "# ==========================================\n",
    "X_text = df[[\"text\"]].copy()\n",
    "# X_text = X_text[51:101].copy()  # テスト用: 50~100 だけ抽出\n",
    "\n",
    "X_text[\"text\"] = (\n",
    "    X_text[\"text\"]\n",
    "    .apply(TextNormalizer.remove_newlines)\n",
    "    .apply(TextNormalizer.replace_links)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# \"keyword\" カラムの前処理\n",
    "# ==========================================\n",
    "X_keyword = df[[\"keyword\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# \"location\" カラムの前処理\n",
    "# ==========================================\n",
    "X_location = df[[\"location\"]].copy()\n",
    "# X_location = X_location[51:101].copy()  # テスト用: 50~100 だけ抽出\n",
    "\n",
    "X_location[\"location\"] = (\n",
    "    X_location[\"location\"]\n",
    "    .apply(TextNormalizer.remove_numbers_and_symbols)\n",
    "    .apply(AdditionalNormalizer.normalize_country_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# モデルの学習と評価\n",
    "# ==========================================\n",
    "\n",
    "# DataFrame として結合 (axis=1 で横方向に結合)\n",
    "X_keyword_and_location = pd.concat([X_keyword, X_location], axis=1)\n",
    "\n",
    "# get_dummies で One-Hot エンコーディング\n",
    "X_keyword_and_location_dummies = pd.get_dummies(\n",
    "    X_keyword_and_location, columns=[\"keyword\", \"location\"], dummy_na=True\n",
    ")  # dummy_na=True で NaN を特徴量化\n",
    "\n",
    "# \"text\" カラムのテキストデータをTF-IDFベクトルに変換\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_text_tfidf = vectorizer.fit_transform(X_text[\"text\"]).toarray()\n",
    "\n",
    "# TF-IDF 特徴量と結合 (こちらは numpy array 同士なので hstack でOK)\n",
    "X = np.hstack((X_keyword_and_location_dummies.values, X_text_tfidf))\n",
    "y = df[\"target\"]\n",
    "\n",
    "# データを訓練セットとテストセットに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 学習されていないモデルオブジェクト\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# モデルの訓練\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# テストセットで予測\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# 精度の計算\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 4564 features, but RandomForestClassifier is expecting 1409 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# TF-IDF特徴量と結合\u001b[39;00m\n\u001b[32m     26\u001b[39m X_prod = np.hstack((X_prod_dummies.values, X_prod_text_tfidf))\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m y_pred_prod = \u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_prod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# 提出用ファイルの作成\u001b[39;00m\n\u001b[32m     30\u001b[39m submission = pd.DataFrame({\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: df_prod[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m\"\u001b[39m: y_pred_prod})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KaggleChallenge/.venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:903\u001b[39m, in \u001b[36mForestClassifier.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    883\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    884\u001b[39m \u001b[33;03m    Predict class for X.\u001b[39;00m\n\u001b[32m    885\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    901\u001b[39m \u001b[33;03m        The predicted classes.\u001b[39;00m\n\u001b[32m    902\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m     proba = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m:\n\u001b[32m    906\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.classes_.take(np.argmax(proba, axis=\u001b[32m1\u001b[39m), axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KaggleChallenge/.venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:945\u001b[39m, in \u001b[36mForestClassifier.predict_proba\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    943\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    944\u001b[39m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m945\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[32m    948\u001b[39m n_jobs, _, _ = _partition_estimators(\u001b[38;5;28mself\u001b[39m.n_estimators, \u001b[38;5;28mself\u001b[39m.n_jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KaggleChallenge/.venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:637\u001b[39m, in \u001b[36mBaseForest._validate_X_predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    635\u001b[39m     ensure_all_finite = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X.indices.dtype != np.intc \u001b[38;5;129;01mor\u001b[39;00m X.indptr.dtype != np.intc):\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KaggleChallenge/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2975\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2975\u001b[39m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2977\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KaggleChallenge/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2839\u001b[39m, in \u001b[36m_check_n_features\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2836\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features != estimator.n_features_in_:\n\u001b[32m-> \u001b[39m\u001b[32m2839\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2840\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2841\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.n_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2842\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: X has 4564 features, but RandomForestClassifier is expecting 1409 features as input."
     ]
    }
   ],
   "source": [
    "# test.csv に対して予測を行う\n",
    "df_prod = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n",
    "X_prod_text_tfidf = vectorizer.transform(df_prod[\"text\"]).toarray()\n",
    "\n",
    "# 訓練データと同じダミー変数構造を作成するため、元の訓練データの列名を取得\n",
    "X_prod = df_prod[[\"keyword\", \"location\"]].copy()\n",
    "X_prod_dummies = pd.get_dummies(X_prod, columns=[\"keyword\", \"location\"], dummy_na=True)\n",
    "\n",
    "# 訓練データで使用した特徴量の列名を取得 (TF-IDF部分を除く)\n",
    "train_feature_cols = pd.get_dummies(\n",
    "    df[[\"keyword\", \"location\"]], columns=[\"keyword\", \"location\"], dummy_na=True\n",
    ").columns\n",
    "\n",
    "# 訓練データに存在してテストデータに存在しない列を特定\n",
    "missing_cols = set(train_feature_cols) - set(X_prod_dummies.columns)\n",
    "\n",
    "# 欠落した列をすべて一度に追加 (パフォーマンス警告を回避)\n",
    "if missing_cols:\n",
    "    missing_df = pd.DataFrame(0, index=X_prod_dummies.index, columns=list(missing_cols))\n",
    "    X_prod_dummies = pd.concat([X_prod_dummies, missing_df], axis=1)\n",
    "\n",
    "# テストデータに存在して訓練データに存在しない列を削除\n",
    "X_prod_dummies = X_prod_dummies[train_feature_cols]\n",
    "\n",
    "# TF-IDF特徴量と結合\n",
    "X_prod = np.hstack((X_prod_dummies.values, X_prod_text_tfidf))\n",
    "y_pred_prod = clf.predict(X_prod)\n",
    "\n",
    "# 提出用ファイルの作成\n",
    "submission = pd.DataFrame({\"id\": df_prod[\"id\"], \"target\": y_pred_prod})\n",
    "submission.to_csv(\"../output/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 869809,
     "sourceId": 17777,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "kagglechallenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
