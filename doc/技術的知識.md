# 技術的な知識

本ドキュメントでは、機械学習やそれに関連する技術的な知識についてまとめる。

## ipynb と py の違い

| 観点                      | `ipynb`（Jupyter Notebook）                      | `py`（Python スクリプト）                |
| ------------------------- | ------------------------------------------------ | ---------------------------------------- |
| **構造**                  | JSON 形式（コード＋出力＋ Markdown を保持）      | プレーンテキスト（Python コードのみ）    |
| **実行方式**              | セル単位で実行、状態を保持                       | ファイル全体を一括実行、状態は保持しない |
| **用途**                  | EDA、実験、プロトタイプ、教育、Kaggle            | 本番コード、ライブラリ、API、バッチ処理  |
| **再現性**                | ❌ セル順序依存で壊れやすい                      | ◎ 安定して再現可能                       |
| **可視化**                | ◎ 結果がそのまま埋め込まれる                     | △ 外部 UI またはログ出力が必要           |
| **ドキュメント性**        | ◎ Markdown 混在で説明しやすい                    | △ 別ファイル（README）必要               |
| **バージョン管理（Git）** | ❌ 差分が巨大・見づらい                          | ◎ 差分がきれいに管理できる               |
| **レビュー性**            | △ 実行状態に依存し理解しにくい                   | ◎ コードのみで明確                       |
| **パッケージ管理**        | `!pip install` で shell 経由                     | `pip install`（シェルから）              |
| **シェルの実行**          | `!ls`, `%%bash` など Magic で可                  | 通常は不可（subprocess が必要）          |
| **学習コスト**            | 低い（直感的）                                   | やや高い（構造化が必要）                 |
| **速度**                  | 基本同じだが Notebook は状態保持で重くなりやすい | 同じだが軽量                             |
| **エコシステム**          | Kaggle / Colab / JupyterLab                      | ほぼ全 Python 環境で利用可能             |
| **本番運用**              | 不向き                                           | 向いている                               |

```json
{
  // Notebook の全セル（コード / Markdown / 出力）
  "cells": [
    {
      // Markdown セル
      "cell_type": "markdown",
      "metadata": {},
      "source": ["# これは説明用の Markdown セルです。"]
    },
    {
      // コードセル
      "cell_type": "code",
      // セルが何回目に実行されたか（未実行なら null）
      "execution_count": 1,
      // コード本体（文字列リスト）
      "source": ["print('Hello, world!')\n", "x = 42\n", "x"],
      // セル実行後の出力（複数ありうる）
      "outputs": [
        {
          // stdout 出力
          "output_type": "stream",
          "name": "stdout",
          "text": ["Hello, world!\n"]
        },
        {
          // 最後に評価された値（Jupyter が自動表示するもの）
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": ["42"]
          }
        }
      ]
    }
  ],

  // Notebook 全体のメタ情報
  "metadata": {
    // 使用するカーネルの情報
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },

    // 言語自体の仕様情報（バージョン / 拡張子など）
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },

  // Notebook ファイルフォーマットのバージョン
  "nbformat": 4,
  "nbformat_minor": 5
}
```

## 機械学習フレームワークについて

### フレームワーク比較表

主要な機械学習フレームワークの特徴比較は以下の通り。
なお、transformers ライブラリは PyTorch / TensorFlow のラッパーであり、NLP タスクに特化したものなので、ここでは除外している。

|              | **PyTorch**            | **TensorFlow**        | **Keras**              | **scikit-learn**   |
| ------------ | ---------------------- | --------------------- | ---------------------- | ------------------ |
| **分類**     | 深層学習               | 深層学習              | 深層学習の高レベル API | 伝統的機械学習     |
| **用途**     | 画像/NLP/研究          | 本番運用/大規模 DL    | 簡易 DL 構築           | 回帰/分類/前処理   |
| **特徴**     | 書きやすい・研究に強い | Google 製・運用に強い | 直感的で簡単           | 軽量・高速         |
| **GPU**      | 対応                   | 対応                  | TensorFlow 依存        | 非対応（基本 CPU） |
| **得意分野** | BERT/GPT など最新 NLP  | モバイル/TPU/大規模   | 初学者向け DL          | SVM, RF, ロジ回帰  |
| **併用**     | TF と併用しない        | PyTorch と併用しない  | TensorFlow とセット    | 深層学習と併用あり |

### 併用について

| 組み合わせ                    | 併用する？ | コメント                   |
| ----------------------------- | ---------- | -------------------------- |
| **PyTorch × TensorFlow**      | ❌ しない  | 役割が競合するため         |
| **Keras × TensorFlow**        | ◎ 常に使う | `tf.keras` が標準          |
| **scikit-learn × PyTorch**    | ◎ よく使う | テキスト前処理・分割など   |
| **scikit-learn × TensorFlow** | ◎ よく使う | パイプラインとの連携が容易 |

### 推奨フレームワーク

NLP Getting Started コンペでの推奨は以下。

**初級：scikit-learn**

- 実装パターン
  - TF-IDF + ロジスティック回帰
  - TF-IDF + SVM
- メリット
  - ランタイムが速い
  - 実装が容易
  - 小データではむしろ強い
- サンプル
  - https://www.kaggle.com/code/philculliton/nlp-getting-started-tutorial
  - スコア 0.78179

**中級：TensorFlow/Keras or PyTorch（自作 NN）**

- 実装パターン
  - LSTM
  - GRU
  - 1D CNN
- メリット
  - 柔軟にモデル設計可能
  - 最近はこれらは非主流だが、学習目的では良い
- サンプル
  - https://www.kaggle.com/code/berkayzkan/rank-8-solution-roberta-large
  - スコア 0.83634

**上級：TensorFlow/PyTorch + Hugging Face Transformers**

- 実装パターン
  - BERT
  - RoBERTa
  - DistilBERT
- メリット
  - 精度最強
- サンプル
  - https://www.kaggle.com/code/dhruv1234/huggingface-tfbertmodel
  - スコア 0.84523

## 言語仕様、補助的ライブラリ・ツール

### リスト/タプル/セットの違い

| 観点               | リスト（list）         | タプル（tuple）        | セット（set）                    |
| ------------------ | ---------------------- | ---------------------- | -------------------------------- |
| **構造**           | 可変長、順序・重複あり | 不変長、順序・重複あり | 順序・重複なし                   |
| **変更可能性**     | 追加・削除・変更が可能 | 追加・削除・変更が不可 | 追加・削除は可能                 |
| **用途**           | データの集約・操作     | 定数・固定データの保持 | 重複除去に便利                   |
| **パフォーマンス** | やや遅い               | やや速い               | 高速（検索が速い）               |
| **メモリ使用量**   | 多い                   | 少ない                 | 中程度                           |
| **シンタックス**   | 角括弧 `[]`            | 丸括弧 `()`            | 波括弧 `{}` （空集合は `set()`） |

### pandas

データ操作ライブラリ。構造化データコンペではほぼ必須。雑にいうと SQL ライクなデータ操作を Python 上で可能にするライブラリ。データフレームは同じデータ型を強要しないが、単一のカラムだけを抽出したものといえるシリーズは同じデータ型を強要する。

[公式ドキュメント](https://pandas.pydata.org/docs/reference/frame.html)。見やすく例も充実している印象。
[get_dummies の参考](https://qiita.com/floatnflow/items/17d00a1f7800bcb5f99e)。

| カテゴリ   | 関数・属性                 | 概要と主な用途                                                                      |
| :--------- | :------------------------- | :---------------------------------------------------------------------------------- |
| **入出力** | `pd.read_csv()`            | CSV ファイルを DataFrame として読み込む。最も頻繁に使われる入力関数。               |
|            | `df.to_csv()`              | DataFrame の内容を CSV ファイルとして出力する。                                     |
| **確認**   | `df.head()` / `df.tail()`  | DataFrame の先頭（デフォルト 5 行）または末尾の行を表示し、データの概要を把握する。 |
|            | `df.info()`                | 各列のデータ型（Dtype）、非 null 値の数、メモリ使用量などの詳細情報を表示する。     |
|            | `df.describe()`            | 数値列の統計的要約（平均、標準偏差、最小・最大値、四分位数など）を計算する。        |
|            | `df['col'].value_counts()` | 指定した列（`'col'`）に含まれる一意な値の出現回数をカウントする。                   |
| **抽出**   | `df['col']`                | 特定の列（Series）を選択する。                                                      |
|            | `df.loc[]` / `df.iloc[]`   | 行と列を、ラベル名（`.loc`）または整数位置（`.iloc`）で選択する。                   |
|            | `df[df['col'] > 10]`       | 条件に基づいて行をフィルタリング（抽出）する。                                      |
| **整形**   | `df.dropna()`              | 欠損値（NaN）を含む行または列を削除する。                                           |
|            | `df.fillna()`              | 欠損値を特定の値（例：0 や平均値）で埋める。                                        |
|            | `df.rename()`              | 列名やインデックス名を変更する。                                                    |
| **集計**   | `df.groupby('col').mean()` | 特定の列（`'col'`）でグループ化し、グループごとに平均などの集計関数を適用する。     |
|            | `df.pivot_table()`         | Excel のピボットテーブルのようにデータを集計・再構成する。                          |
| **結合**   | `pd.merge()`               | 複数の DataFrame を、共通の列をキーとして結合（JOIN）する。                         |
|            | `pd.concat()`              | 複数の DataFrame を、縦（行）または横（列）に連結する。                             |

### NumPy

行列演算を中心とした数学的な演算処理に使うライブラリ。全ての要素が同じデータ型をもつ点で、Python 標準のリストなどと異なる。内部で C の資産を使用しており実行が速く、標準のリストより直感的な行列計算を使用できる。

[公式ドキュメント](https://numpy.org/doc/stable/reference/arrays.ndarray.html)。見やすいがちょっと小難しいかも。

| カテゴリ     | 関数・属性                               | 概要と主な用途                                                         |
| :----------- | :--------------------------------------- | :--------------------------------------------------------------------- |
| **配列生成** | `np.array([..])`                         | Python のリストやタプルから`ndarray`を生成する。                       |
|              | `np.zeros(shape)` / `np.ones(shape)`     | 全てが 0 または 1 の指定した形状（`shape`）の配列を生成する。          |
|              | `np.arange(start, stop, step)`           | 指定範囲で等差数列を生成する。                                         |
|              | `np.linspace(start, stop, num)`          | 指定範囲を均等に分割した要素を持つ配列を生成する。                     |
|              | `np.random.rand()` / `np.random.randn()` | 乱数で配列を生成する（機械学習の重み初期化などで多用）。               |
| **形状操作** | `array.shape`                            | 配列の形状（タプル）を取得する。                                       |
|              | `array.reshape(new_shape)`               | 配列の要素数を変えずに形状を変更する。                                 |
|              | `array.T` / `np.transpose(array)`        | 配列を転置（行と列を入れ替え）する。行列計算の際に頻繁に使う。         |
| **集計**     | `np.sum(array, axis=...)`                | 配列内の要素の合計を計算する。`axis`指定で行/列方向の合計も可能。      |
|              | `np.mean()`, `np.std()`                  | 配列内の要素の平均、標準偏差などを計算する。                           |
|              | `np.dot(a, b)` / `a @ b`                 | 配列 a と b の行列積（内積）を計算する。機械学習の基盤となる演算。     |
| **演算**     | `np.exp()`, `np.log()`, `np.sqrt()`      | 配列の各要素に対して、指数関数、対数、平方根などの数学関数を適用する。 |
|              | `array > 5`                              | 配列の各要素に対して条件判定を行い、真偽値（True/False）の配列を返す。 |

### functools.lru_cache

以下のような関数デコレータで、関数の引数と戻り値をキャッシュする。一度実行された引数はキャッシュされた戻り値が返されるようになり、実際の関数処理がスキップされる。計算コストの高い関数に対して有効。一部処理において非常に大きな時間削減が可能。

```python
from functools

@functools.lru_cache(maxsize=1024) # キャッシュを何回分保持するか指定
def take_long_time_function(x):
    # 時間のかかる処理
    return result
```

## !pip install と %pip install の違い

**結論:** 基本的に `%pip install` を使う ([参考](https://www.kaggle.com/code/matinmahmoudi/complete-guide-to-pip-commands-a-to-z))

### !pip

`!` は「シェルコマンド実行」のための IPython 構文。`!pip install xxx` は Python カーネル（実行環境）とは無関係にシェル上で pip が実行されるので、以下が起こりうる。

```python
!pip install numpy
import numpy  # インポートエラー
```

### %pip

`%` は「マジックコマンド実行」のための IPython 構文。`%pip install xxx` は現在の Python カーネル（実行環境）に紐づいた pip が実行されるので、上記のような問題が起こらない。

## ipynb の import

重要な基本として、ipynb ファイルをモジュールとして import することはできない。基本的に再利用するコードは py ファイルとして保存し、ipynb から import して使う。

### 独自モジュールの import

Notebook は一度インポートしたモジュールをキャッシュするため、my_module.py のコードを書き換えて保存しても、Notebook 側のセルを再実行するだけでは変更が反映されない。カーネル（実行プロセス）再起動が必要。
自動リロードを有効にするマジックコマンドを入れておくと、my_module.py のコードを書き換えて保存した際に自動的に再インポートされるようになる。

```python
%load_ext autoreload
%autoreload 2

import my_module
```

## 主要なデータ形式

### 量的データ (定量的データ)

- 定義: 数値で表され、計算や統計処理が可能なデータです。
- 種類:
  - 離散データ: 個数や回数など、飛び飛びの値をとるデータ（例: クラスの人数、テストの点数）。
  - 連続データ: 身長、体重、時間など、小数点以下も含む連続した値をとるデータ（例: 気温、売上金額）。

### 非構造化データ

- 定義: カテゴリデータや表形式のような決まった構造を持たないデータです。現代のデジタルデータの大部分を占めます。
- 種類:
  - テキストデータ: 文章、レビュー、SNS の投稿、電子メールなど。
  - 画像・音声データ: 写真、イラスト、音楽ファイル、録音データなど。
  - 動画データ: YouTube の動画など。

### 半構造化データ

- 定義: 完全に構造化されているわけではないが、データの中にタグやマークアップによって一定の意味や階層構造が含まれているデータです。
- 種類:
  - XML
  - JSON
  - HTML

## EDA（探索的データ解析）

EDA は、**データの本質を理解し、前処理とモデル戦略を決定する**ための初期分析フェーズです。
「Garbage In, Garbage Out」（質の悪いデータからは質の悪い結果しか得られない）を防ぎ、モデルの精度と信頼性を向上させるための土台となります。

### 目的（Why）

- **構造把握:** データセットのサイズ、変数の型、欠損値の分布を知る。
- **品質評価:** データの異常値・外れ値、エラー、不整合性を発見する。
- **関係性発見:** 特徴量（X）と目的変数（Y）の関係性や、特徴量同士の相関を見つける。
- **指針決定:** 前処理（クリーニング、変換）やモデル選択の方向性を決める。

### 手法（How）

| カテゴリ       | 手法（視覚化）              | 目的                                                           |
| :------------- | :-------------------------- | :------------------------------------------------------------- |
| **単変量分析** | **ヒストグラム** / KDE      | 個々の**数値変数の分布**（偏り、形状）を確認。                 |
|                | **棒グラフ**                | **カテゴリ変数**の**頻度**を確認。                             |
|                | **箱ひげ図 (Box Plot)**     | 分布の要約と**外れ値**の検出。                                 |
| **多変量分析** | **散布図**                  | 2 つの**数値変数間の関係**（線形性など）を確認。               |
|                | **相関行列 (ヒートマップ)** | 全ての変数ペアの**相関係数**を視覚化し、多重共線性をチェック。 |
|                | **クロス集計**              | 2 つのカテゴリ変数の**関連性**を確認。                         |

## データ前処理

データ前処理は、一般的に「**データクリーニング**」「**データ変換**」「**構造演算（データ削減と統合）**」の 3 つの主要な段階に分けられます。

ご要望に応じて、データ前処理の主要なステップを 3 つのカテゴリに分け、箇条書き形式で簡潔にまとめます。

### 1. データクリーニング（Data Cleaning）

データの**品質**を向上させ、不正確さや不整合を取り除くステップです。

- **欠損値の処理**
  - **概要:** データに存在する空欄や`NaN`を適切に埋める、あるいは無視する。
  - **主な作業:** 行や列の削除、または平均値などで補完（Imputation）。
- **外れ値の処理**
  - **概要:** データ分布から極端にかけ離れた異常な値（外れ値）を特定し、対処する。
  - **主な作業:** 検出した外れ値の削除、または許容範囲内に丸める（クリッピング）。
- **ノイズの除去**
  - **概要:** 誤った記録や測定誤差などによる「ノイズ」を取り除く。
  - **主な作業:** **平滑化**（Binning）によるデータ区間での平均値等での置換。
- **データ不整合性の対処**
  - **概要:** 同じ意味を持つが表記が異なるデータ（例：「(株)」と「株式会社」）を統一する。
  - **主な作業:** データの標準化、表記の統一。

### 2. データ変換（Data Transformation）

データをモデルが学習しやすい形式やスケールに**変換**するステップです。

- **カテゴリ変数のエンコーディング**
  - **概要:** 文字列のカテゴリデータ（例：色、性別）を数値データに変換する。
  - **主な作業:** One-Hot Encoding（順序なし）、Label Encoding（順序あり）への変換。
- **特徴量のスケーリング**
  - **概要:** 特徴量間のスケールの違いを解消し、モデルの学習効率と精度を高める。
  - **主な作業:** 標準化（平均 0、標準偏差 1）や正規化（データの範囲を $[0, 1]$ に変換）。
- **特徴量エンジニアリング**
  - **概要:** 既存の特徴量から、より有用で意味のある新しい特徴量を作成する。
  - **主な作業:** 複数の特徴量の組み合わせ（例：BMI の計算）、日付データからの情報抽出。

### 3. 構造演算（Structure Operations）

データセットのサイズや次元を調整したり、複数のソースからのデータを**統合**したりするステップです。

- **次元削減 (Dimensionality Reduction)**
  - **概要:** 特徴量（列）の数を減らし、計算コストを下げ、過学習を防ぐ。
  - **主な作業:** 特徴量選択、主成分分析 (PCA) などによる特徴量抽出。
- **データ圧縮**
  - **概要:** データ量を削減し、メモリ消費を抑え、I/O を高速化する。
  - **主な作業:** 疎なデータ（Sparse Data）の効率的な格納（疎行列形式など）。
- **データ統合 (Data Integration)**
  - **概要:** 複数の異なるデータソースから集めたデータを一つのデータセットに結合する。
  - **主な作業:** 共通キーを用いたデータセットの結合（JOIN）、データのスキーマ調整や矛盾の解決。

## ライブラリと次元数

基本的に、機械学習ライブラリごとに扱えるデータの次元数が異なる。

| ライブラリ       | 扱える次元数の目安                 |
| ---------------- | ---------------------------------- |
| scikit-learn     | 1D（ベクトル）、2D（行列）         |
| TensorFlow/Keras | 1D〜（ベクトル、行列、画像、動画） |
| PyTorch          | 1D〜（ベクトル、行列、画像、動画） |

## ハイパーパラメータとパラメータ

- **パラメータ (Parameters)**:
  - モデルが学習する値。例えば、線形回帰モデルの重みやバイアスなど。
  - 学習データから直接最適化される。
- **ハイパーパラメータ (Hyperparameters)**:
  - モデルの学習プロセスや構造を制御する設定値。例えば、学習率、決定木の深さ、バッチサイズなど。
  - 学習前に手動で設定され、モデルの性能に大きな影響を与える。
  - モデルの精度向上と過学習防止のバランスを取るために重要。

```python
RandomForestClassifier(
    max_depth=15, # 木の深さ、真っ先に変更を試みる対象
    min_samples_leaf=2, # 葉ノードの最小サンプル数、真っ先に変更を試みる対象
    n_estimators=100, # 決定木の数、増やせば大抵精度向上するが、計算コストと相談
    max_features="sqrt", # 各決定木で使用する特徴量の数、基本デフォルトで十分だが特徴量多い（数千～）場合は調整を検討
    class_weight="balanced",  # クラス（モデルの予測対象）が不均衡な場合に有効
    random_state=42,
    n_jobs=-1
)
```

## モデルの種類

### 決定木ベース

シンプルで解釈しやすいモデル。特徴量の重要度がわかりやすく、カテゴリデータの扱いに強い。
メリットは、扱いやすさと解釈性の高さ。デメリットは、単一の決定木は過学習しやすい点。
この過学習を解決するために、レアカテゴリに足切りを入れたり、レアカテゴリをまとめたりする前処理が有効。

決定木ベースのモデルには以下のようなものがある。

**RandomForest**
決定木を多数組み合わせたアンサンブル学習モデル。過学習に強く、扱いやすい。scikit-learn で簡単に利用可能。

**GradientBoosting**
弱い学習器（通常は決定木）を逐次的に組み合わせて強力なモデルを構築する手法。精度が高いが、計算コストが高い。XGBoost や LightGBM などの実装が有名。
